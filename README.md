# ML Service

Микросервисная система для выполнения ML-предсказаний с использованием очередей сообщений.

## Архитектура

Система состоит из следующих микросервисов:

1. **App** - REST API сервис на FastAPI для приема и обработки запросов, а также просмотра результатов.
2. **Bot** - Telegram бот для взаимодействия с пользователями.
3. **ML Worker** - Сервис для выполнения ML предсказаний, обрабатывающий задания из очереди.
4. **RabbitMQ** - Брокер сообщений для обмена данными между сервисами.
5. **PostgreSQL** - База данных для хранения информации о пользователях, предсказаниях и транзакциях.
6. **Nginx** - Прокси-сервер для маршрутизации HTTP-запросов.

## Схема работы

1. Пользователь делает API запрос через web интерфейс или Telegram бота
2. Запрос обрабатывается и формируется задание на предикт
3. Задание попадает в очередь RabbitMQ (`ml_tasks`)
4. Один из ML Worker берет задание и выполняет предикт
5. Результат сохраняется в базе данных и отправляется в очередь результатов (`ml_results`)
6. Пользователь может получить результат через API или получить уведомление через Telegram бота

## Структура проекта

```
.
├── docker-compose.yaml   # Основная конфигурация Docker Compose
├── start.py              # Скрипт для управления системой
├── services/
│   ├── app/              # FastAPI приложение с REST API
│   │   ├── Dockerfile
│   │   ├── main.py       # Основной код FastAPI
│   │   ├── requirements.txt
│   │   └── .env
│   ├── bot/              # Telegram бот
│   │   ├── Dockerfile
│   │   ├── bot.py        # Основной код Telegram бота
│   │   ├── requirements.txt
│   │   └── .env
│   ├── ml_worker/        # ML Worker для выполнения предсказаний
│   │   ├── Dockerfile
│   │   ├── worker.py     # Код для обработки ML задач
│   │   ├── requirements.txt
│   │   └── .env
│   └── nginx/            # Nginx конфигурация
│       └── nginx.conf
```

## Взаимодействие через RabbitMQ

В системе используется две основные очереди:

- **ml_tasks** - очередь для заданий на предсказание
- **ml_results** - очередь для результатов предсказаний

RabbitMQ настроен в режиме "один издатель - несколько слушателей", что позволяет:
- Публиковать задания из API и Telegram бота в общую очередь
- Выполнять предсказания на нескольких ML Worker параллельно
- Динамически масштабировать количество воркеров в зависимости от нагрузки

## Масштабирование ML Workers

Система поддерживает горизонтальное масштабирование ML Worker:

- По умолчанию запускается 3 воркера
- Для изменения количества воркеров можно использовать параметр `--workers` в скрипте `start.py`
- Или напрямую через Docker Compose: `docker-compose up -d --scale ml-worker=5`

Каждый воркер получает уникальный ID, что позволяет отслеживать, какой именно воркер обработал предсказание.

## Запуск системы

### Быстрый запуск

Для быстрого запуска всей системы достаточно выполнить:

```bash
docker-compose up -d
```

Это запустит все сервисы и создаст 3 экземпляра ML Worker.

### С помощью скрипта start.py

Для удобства управления системой существует скрипт `start.py`, который предоставляет интерактивный режим и командную строку:

```bash
# Запуск интерактивного режима
python start.py

# Запуск всех микросервисов
python start.py --start

# Запуск с указанием количества ML воркеров
python start.py --start --workers 5

# Остановка всех микросервисов
python start.py --stop

# Проверка статуса
python start.py --status

# Просмотр логов
python start.py --logs
```

### Напрямую через Docker Compose

Для запуска системы можно использовать Docker Compose:

```bash
docker-compose up -d
```

Для запуска системы с несколькими ML Worker:

```bash
docker-compose up -d --scale ml-worker=5
```

## Тестирование системы

Вы можете протестировать систему следующими способами:

### Через REST API

1. Откройте Swagger документацию по адресу http://localhost/docs
2. Зарегистрируйтесь или используйте тестового пользователя (логин: test, пароль: test)
3. Получите токен через `/token`
4. Отправьте запрос на предсказание через `/predictions/predict`
5. Проверьте результат через `/predictions/{prediction_id}`

### Через Telegram бота

1. Запустите бота командой `/start`
2. Используйте команду `/predict` для отправки текста на предсказание
3. Дождитесь уведомления о готовности результата

## Endpoints

API доступен по адресу http://localhost:80 и предоставляет следующие endpoints:

- `/docs` - Swagger документация API
- `/users` - Регистрация пользователя
- `/token` - Получение токена аутентификации
- `/predictions/predict` - Отправка запроса на предсказание
- `/predictions/{prediction_id}` - Получение результата предсказания
- `/predictions` - Получение истории предсказаний
- `/balance` - Получение баланса пользователя
- `/health` - Проверка работоспособности сервиса

## Telegram бот

Перед использованием Telegram бота необходимо указать токен бота в файле `services/bot/.env`:

```
TELEGRAM_TOKEN=your_telegram_bot_token
```

Команды бота:
- `/start` - Начало работы с ботом
- `/predict` - Сделать предсказание
- `/balance` - Узнать баланс
- `/history` - История предсказаний

## Мониторинг и управление

### RabbitMQ

Веб-интерфейс RabbitMQ доступен по адресу http://localhost:15672 
(логин: guest, пароль: guest).

Здесь вы можете:
- Отслеживать очереди и их состояние
- Смотреть статистику обработки сообщений
- Проверять количество активных соединений

### Просмотр логов

Для просмотра логов отдельных сервисов:

```bash
docker-compose logs app     # Логи FastAPI приложения
docker-compose logs bot     # Логи Telegram бота
docker-compose logs ml-worker  # Логи ML Worker
docker-compose logs rabbitmq   # Логи RabbitMQ
```