# Развертывание ML сервиса

## Архитектура проекта

Проект состоит из следующих компонентов:

1. **API-Сервис (app)** - FastAPI приложение, предоставляющее REST API для взаимодействия с ML сервисом. 
   - Обрабатывает входящие запросы
   - Отправляет задачи на предсказание в RabbitMQ
   - Предоставляет API для получения результатов предсказаний
   - Управляет аутентификацией и авторизацией пользователей

2. **Телеграм-бот (telegram-bot)** - Интерфейс для пользователей через Telegram.
   - Управляет пользовательскими сессиями
   - Отправляет запросы на предсказание через RabbitMQ
   - Отображает результаты предсказаний пользователю

3. **ML-Воркер (ml-worker)** - Обработчик ML задач.
   - Получает задачи из очереди RabbitMQ
   - Выполняет валидацию входных данных
   - Выполняет предсказания с использованием ML моделей
   - Сохраняет результаты в базу данных

4. **База данных (PostgreSQL)** - Хранение данных о пользователях, предсказаниях и т.д.

5. **RabbitMQ** - Брокер сообщений для асинхронной обработки задач.
   - Очередь 'ml_tasks' для задач предсказания

6. **Web-прокси (NGINX)** - Прокси-сервер для обработки HTTP-запросов.

## Структура директорий

```
deployment/
├── app/              # Файлы для API-сервиса
│   ├── Dockerfile    # Dockerfile для сборки API-сервиса
│   ├── .env          # Переменные окружения для API-сервиса
│   └── requirements.txt # Зависимости для API-сервиса
├── bot/              # Файлы для Telegram-бота
│   ├── Dockerfile    # Dockerfile для сборки Telegram-бота
│   ├── .env          # Переменные окружения для Telegram-бота
│   └── requirements.txt # Зависимости для Telegram-бота
├── worker/           # Файлы для ML-воркера
│   ├── Dockerfile    # Dockerfile для сборки ML-воркера
│   ├── .env          # Переменные окружения для ML-воркера
│   └── requirements.txt # Зависимости для ML-воркера
├── nginx/            # Конфигурация NGINX
│   └── nginx.conf    # Конфигурационный файл NGINX
└── docker-compose.yaml # Файл для запуска всех сервисов
```

## Запуск сервисов

Для запуска всех сервисов используйте команду:

```bash
docker-compose up
```

При первом запуске будут автоматически созданы образы для всех сервисов. 

Для запуска в фоновом режиме:

```bash
docker-compose up -d
```

## Проверка работы сервисов

1. **API-сервис**: Доступен по адресу http://localhost/api/docs
2. **RabbitMQ**: Веб-интерфейс доступен по адресу http://localhost:15672 (логин: guest, пароль: guest)
3. **Телеграм-бот**: Найдите своего бота в Telegram и начните диалог с ним

## Масштабирование ML-воркеров

В конфигурации docker-compose.yaml настроено автоматическое создание 3 экземпляров ML-воркера для параллельной обработки задач. При необходимости количество воркеров можно изменить, отредактировав параметр `scale` в файле docker-compose.yaml:

```yaml
ml-worker:
  ...
  scale: 3  # Изменить на нужное количество экземпляров
```

## Тестирование системы

1. Отправьте запрос через API-сервис:
   ```bash
   curl -X POST "http://localhost/predictions/predict" -H "Content-Type: application/json" -d '{"data": {"text": "Какой-то текст для предсказания"}}'
   ```

2. Или используйте Telegram-бота для отправки запроса на предсказание 